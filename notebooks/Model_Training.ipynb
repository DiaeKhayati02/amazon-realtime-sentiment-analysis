{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark found by findspark.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "try:\n",
    "    findspark.init()\n",
    "    print(\"Spark found by findspark.\")\n",
    "except:\n",
    "    print(\"Spark not found by findspark.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java version \"1.8.0_431\"\n",
      "Java(TM) SE Runtime Environment (build 1.8.0_431-b10)\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 25.431-b10, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat_ws\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import split, trim, size\n",
    "from pyspark.sql.functions import length\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('../data/Musical_instruments_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Amazon Sentiment Analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+--------------------+--------+--------------------+-------+--------------------+--------------+-----------+\n",
      "|    reviewerID|      asin|        reviewerName| helpful|          reviewText|overall|             summary|unixReviewTime| reviewTime|\n",
      "+--------------+----------+--------------------+--------+--------------------+-------+--------------------+--------------+-----------+\n",
      "|A2IBPI20UZIR0U|1384719342|cassandra tu \"Yea...|  [0, 0]|Not much to write...|    5.0|                good|    1393545600|02 28, 2014|\n",
      "|A14VAT5EAX3D9S|1384719342|                Jake|[13, 14]|The product does ...|    5.0|                Jake|    1363392000|03 16, 2013|\n",
      "|A195EZSQDW3E21|1384719342|Rick Bennette \"Ri...|  [1, 1]|The primary job o...|    5.0|It Does The Job Well|    1377648000|08 28, 2013|\n",
      "|A2C00NNG1ZQQG2|1384719342|RustyBill \"Sunday...|  [0, 0]|Nice windscreen p...|    5.0|GOOD WINDSCREEN F...|    1392336000|02 14, 2014|\n",
      "| A94QU4C90B1AX|1384719342|       SEAN MASLANKA|  [0, 0]|This pop filter i...|    5.0|No more pops when...|    1392940800|02 21, 2014|\n",
      "+--------------+----------+--------------------+--------+--------------------+-------+--------------------+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df = spark.read.csv(\"../data/Musical_instruments_reviews.csv\", header=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna({'reviewText': 'Missing'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'reviewText' and 'summary' into a new column 'reviews'\n",
    "df = df.withColumn('reviews', concat_ws('', col('reviewText'), col('summary')))\n",
    "\n",
    "# Drop the original 'reviewText' and 'summary' columns\n",
    "df = df.drop('reviewText', 'summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+--------------------+--------+-------+--------------+-----------+--------------------+\n",
      "|    reviewerID|      asin|        reviewerName| helpful|overall|unixReviewTime| reviewTime|             reviews|\n",
      "+--------------+----------+--------------------+--------+-------+--------------+-----------+--------------------+\n",
      "|A2IBPI20UZIR0U|1384719342|cassandra tu \"Yea...|  [0, 0]|    5.0|    1393545600|02 28, 2014|Not much to write...|\n",
      "|A14VAT5EAX3D9S|1384719342|                Jake|[13, 14]|    5.0|    1363392000|03 16, 2013|The product does ...|\n",
      "|A195EZSQDW3E21|1384719342|Rick Bennette \"Ri...|  [1, 1]|    5.0|    1377648000|08 28, 2013|The primary job o...|\n",
      "|A2C00NNG1ZQQG2|1384719342|RustyBill \"Sunday...|  [0, 0]|    5.0|    1392336000|02 14, 2014|Nice windscreen p...|\n",
      "| A94QU4C90B1AX|1384719342|       SEAN MASLANKA|  [0, 0]|    5.0|    1392940800|02 21, 2014|This pop filter i...|\n",
      "|A2A039TZMZHH9Y|B00004Y2UT| Bill Lewey \"blewey\"|  [0, 0]|    5.0|    1356048000|12 21, 2012|So good that I bo...|\n",
      "|A1UPZM995ZAH90|B00004Y2UT|               Brian|  [0, 0]|    5.0|    1390089600|01 19, 2014|I have used monst...|\n",
      "| AJNFQI3YR6XJ5|B00004Y2UT|   Fender Guy \"Rick\"|  [0, 0]|    3.0|    1353024000|11 16, 2012|I now use this ca...|\n",
      "|A3M1PLEYNDEYO8|B00004Y2UT|     G. Thomas \"Tom\"|  [0, 0]|    5.0|    1215302400| 07 6, 2008|Perfect for my Ep...|\n",
      "| AMNTZU1YQN1TH|B00004Y2UT|         Kurt Robair|  [0, 0]|    5.0|    1389139200| 01 8, 2014|Monster makes the...|\n",
      "|A2NYK9KWFMJV4Y|B00004Y2UT|Mike Tarrani \"Jaz...|  [6, 6]|    5.0|    1334793600|04 19, 2012|Monster makes a w...|\n",
      "|A35QFQI0M46LWO|B00005ML71|       Christopher C|  [0, 0]|    4.0|    1398124800|04 22, 2014|I got it to have ...|\n",
      "|A2NIT6BKW11XJQ|B00005ML71|                 Jai|  [0, 0]|    3.0|    1384646400|11 17, 2013|If you are not us...|\n",
      "|A1C0O09LOLVI39|B00005ML71|             Michael|  [0, 0]|    5.0|    1371340800|06 16, 2013|I love it, I used...|\n",
      "|A17SLR18TUMULM|B00005ML71|         Straydogger|  [0, 0]|    5.0|    1356912000|12 31, 2012|I bought this to ...|\n",
      "|A2PD27UKAD3Q00|B00005ML71|Wilhelmina Zeitge...|  [0, 0]|    2.0|    1376697600|08 17, 2013|I bought this to ...|\n",
      "| AKSFZ4G1AXYFC|B000068NSX|        C.E. \"Frank\"|  [0, 0]|    4.0|    1376352000|08 13, 2013|This Fender cable...|\n",
      "| A67OJZLHBBUQ9|B000068NSX|Charles F. Marks ...|  [0, 0]|    5.0|    1373328000| 07 9, 2013|wanted it just on...|\n",
      "|A2EZWZ8MBEDOLN|B000068NSX|              Charlo|  [3, 3]|    5.0|    1363564800|03 18, 2013|I've been using t...|\n",
      "|A1CL807EOUPVP1|B000068NSX|             GunHawk|  [0, 0]|    5.0|    1375833600| 08 7, 2013|Fender cords look...|\n",
      "+--------------+----------+--------------------+--------+-------+--------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the sentiment function\n",
    "def sent(overall):\n",
    "    if overall == 3:\n",
    "        return 'Neutral'\n",
    "    elif overall > 3:\n",
    "        return 'Positive'\n",
    "    else:\n",
    "        return 'Negative'\n",
    "\n",
    "# Register the UDF\n",
    "sent_udf = udf(sent, StringType())\n",
    "\n",
    "# Apply the UDF to create the 'Sentiment' column\n",
    "df = df.withColumn('Sentiment', sent_udf(df['overall']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+--------------------+--------+-------+--------------+-----------+--------------------+---------+\n",
      "|    reviewerID|      asin|        reviewerName| helpful|overall|unixReviewTime| reviewTime|             reviews|Sentiment|\n",
      "+--------------+----------+--------------------+--------+-------+--------------+-----------+--------------------+---------+\n",
      "|A2IBPI20UZIR0U|1384719342|cassandra tu \"Yea...|  [0, 0]|    5.0|    1393545600|02 28, 2014|Not much to write...| Positive|\n",
      "|A14VAT5EAX3D9S|1384719342|                Jake|[13, 14]|    5.0|    1363392000|03 16, 2013|The product does ...| Positive|\n",
      "|A195EZSQDW3E21|1384719342|Rick Bennette \"Ri...|  [1, 1]|    5.0|    1377648000|08 28, 2013|The primary job o...| Positive|\n",
      "|A2C00NNG1ZQQG2|1384719342|RustyBill \"Sunday...|  [0, 0]|    5.0|    1392336000|02 14, 2014|Nice windscreen p...| Positive|\n",
      "| A94QU4C90B1AX|1384719342|       SEAN MASLANKA|  [0, 0]|    5.0|    1392940800|02 21, 2014|This pop filter i...| Positive|\n",
      "|A2A039TZMZHH9Y|B00004Y2UT| Bill Lewey \"blewey\"|  [0, 0]|    5.0|    1356048000|12 21, 2012|So good that I bo...| Positive|\n",
      "|A1UPZM995ZAH90|B00004Y2UT|               Brian|  [0, 0]|    5.0|    1390089600|01 19, 2014|I have used monst...| Positive|\n",
      "| AJNFQI3YR6XJ5|B00004Y2UT|   Fender Guy \"Rick\"|  [0, 0]|    3.0|    1353024000|11 16, 2012|I now use this ca...|  Neutral|\n",
      "|A3M1PLEYNDEYO8|B00004Y2UT|     G. Thomas \"Tom\"|  [0, 0]|    5.0|    1215302400| 07 6, 2008|Perfect for my Ep...| Positive|\n",
      "| AMNTZU1YQN1TH|B00004Y2UT|         Kurt Robair|  [0, 0]|    5.0|    1389139200| 01 8, 2014|Monster makes the...| Positive|\n",
      "+--------------+----------+--------------------+--------+-------+--------------+-----------+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+--------------------+--------+-------+--------------+--------------------+---------+----+-----+---+\n",
      "|    reviewerID|      asin|        reviewerName| helpful|overall|unixReviewTime|             reviews|Sentiment|year|month|day|\n",
      "+--------------+----------+--------------------+--------+-------+--------------+--------------------+---------+----+-----+---+\n",
      "|A2IBPI20UZIR0U|1384719342|cassandra tu \"Yea...|  [0, 0]|    5.0|    1393545600|Not much to write...| Positive|2014|   02| 28|\n",
      "|A14VAT5EAX3D9S|1384719342|                Jake|[13, 14]|    5.0|    1363392000|The product does ...| Positive|2013|   03| 16|\n",
      "|A195EZSQDW3E21|1384719342|Rick Bennette \"Ri...|  [1, 1]|    5.0|    1377648000|The primary job o...| Positive|2013|   08| 28|\n",
      "|A2C00NNG1ZQQG2|1384719342|RustyBill \"Sunday...|  [0, 0]|    5.0|    1392336000|Nice windscreen p...| Positive|2014|   02| 14|\n",
      "| A94QU4C90B1AX|1384719342|       SEAN MASLANKA|  [0, 0]|    5.0|    1392940800|This pop filter i...| Positive|2014|   02| 21|\n",
      "+--------------+----------+--------------------+--------+-------+--------------+--------------------+---------+----+-----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Split 'reviewTime' into 'date' and 'year'\n",
    "df = df.withColumn('date', split(df['reviewTime'], ',').getItem(0))\n",
    "df = df.withColumn('year', trim(split(df['reviewTime'], ',').getItem(1)))\n",
    "\n",
    "# Split 'date' into 'month' and 'day'\n",
    "df = df.withColumn('month', split(df['date'], ' ').getItem(0))\n",
    "df = df.withColumn('day', split(df['date'], ' ').getItem(1))\n",
    "\n",
    "# Drop 'reviewTime' and 'date' columns\n",
    "df = df.drop('reviewTime', 'date')\n",
    "\n",
    "# Show the result\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('reviewerName', 'unixReviewTime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_review_udf = udf(clean_review, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('reviews', clean_review_udf(df['reviews']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+--------+-------+--------------------+---------+----+-----+---+\n",
      "|    reviewerID|      asin| helpful|overall|             reviews|Sentiment|year|month|day|\n",
      "+--------------+----------+--------+-------+--------------------+---------+----+-----+---+\n",
      "|A2IBPI20UZIR0U|1384719342|  [0, 0]|    5.0|not much to write...| Positive|2014|   02| 28|\n",
      "|A14VAT5EAX3D9S|1384719342|[13, 14]|    5.0|the product does ...| Positive|2013|   03| 16|\n",
      "|A195EZSQDW3E21|1384719342|  [1, 1]|    5.0|the primary job o...| Positive|2013|   08| 28|\n",
      "|A2C00NNG1ZQQG2|1384719342|  [0, 0]|    5.0|nice windscreen p...| Positive|2014|   02| 14|\n",
      "| A94QU4C90B1AX|1384719342|  [0, 0]|    5.0|this pop filter i...| Positive|2014|   02| 21|\n",
      "+--------------+----------+--------+-------+--------------------+---------+----+-----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words= ['yourselves', 'between', 'whom', 'itself', 'is', \"she's\", 'up', 'herself', 'here', 'your', 'each', \n",
    "             'we', 'he', 'my', \"you've\", 'having', 'in', 'both', 'for', 'themselves', 'are', 'them', 'other',\n",
    "             'and', 'an', 'during', 'their', 'can', 'yourself', 'she', 'until', 'so', 'these', 'ours', 'above', \n",
    "             'what', 'while', 'have', 're', 'more', 'only', \"needn't\", 'when', 'just', 'that', 'were', \"don't\", \n",
    "             'very', 'should', 'any', 'y', 'isn', 'who',  'a', 'they', 'to', 'too', \"should've\", 'has', 'before',\n",
    "             'into', 'yours', \"it's\", 'do', 'against', 'on',  'now', 'her', 've', 'd', 'by', 'am', 'from', \n",
    "             'about', 'further', \"that'll\", \"you'd\", 'you', 'as', 'how', 'been', 'the', 'or', 'doing', 'such',\n",
    "             'his', 'himself', 'ourselves',  'was', 'through', 'out', 'below', 'own', 'myself', 'theirs', \n",
    "             'me', 'why', 'once',  'him', 'than', 'be', 'most', \"you'll\", 'same', 'some', 'with', 'few', 'it',\n",
    "             'at', 'after', 'its', 'which', 'there','our', 'this', 'hers', 'being', 'did', 'of', 'had', 'under',\n",
    "             'over','again', 'where', 'those', 'then', \"you're\", 'i', 'because', 'does', 'all']\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    words = text.split()\n",
    "    filtered = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "remove_stopwords_udf = udf(remove_stopwords, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('reviews', remove_stopwords_udf(df['reviews']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polarity(text):\n",
    "    try:\n",
    "        return float(TextBlob(str(text)).sentiment.polarity)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "polarity_udf = udf(get_polarity, FloatType())\n",
    "df = df.withColumn('polarity', polarity_udf(df['reviews']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+----------+----------+\n",
      "|             reviews|   polarity|review_len|word_count|\n",
      "+--------------------+-----------+----------+----------+\n",
      "|not much write bu...|       0.25|       162|        25|\n",
      "|product exactly q...|0.014285714|       356|        55|\n",
      "|primary job devic...|     0.1675|       315|        48|\n",
      "|nice windscreen p...| 0.33333334|       169|        22|\n",
      "|pop filter great ...|        0.8|       136|        21|\n",
      "+--------------------+-----------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('review_len', length(df['reviews']))\n",
    "df = df.withColumn('word_count', size(split(df['reviews'], ' ')))\n",
    "\n",
    "df.select('reviews', 'polarity', 'review_len', 'word_count').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_review = df.filter(df[\"Sentiment\"] == 'Positive').na.drop()\n",
    "neutral_review  = df.filter(df[\"Sentiment\"] == 'Neutral').na.drop()\n",
    "negative_review = df.filter(df[\"Sentiment\"] == 'Negative').na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+----------+-------+--------------------+---------+----+-----+---+------------+----------+----------+\n",
      "|    reviewerID|      asin|   helpful|overall|             reviews|Sentiment|year|month|day|    polarity|review_len|word_count|\n",
      "+--------------+----------+----------+-------+--------------------+---------+----+-----+---+------------+----------+----------+\n",
      "|A2PD27UKAD3Q00|B00005ML71|    [0, 0]|    2.0|bought use keyboa...| Negative|2013|   08| 17|  0.26944444|       430|        63|\n",
      "|A12ABV9NU02O29|B000068NW5|    [2, 2]|    2.0|didnt expect cabl...| Negative|2011|   07|  6|-0.018707482|       281|        41|\n",
      "|A1L7M2JXN4EZCR|B000068NW5|    [0, 0]|    1.0|hums crackles thi...| Negative|2014|   02|  9|         0.6|       183|        29|\n",
      "|A3UD50M7M72150|B000068NW5|    [0, 0]|    1.0|im procheapo hate...| Negative|2014|   03| 14|       -0.25|       106|        16|\n",
      "|A1W3CEEQBJ4GTN|B000068NZC|    [0, 0]|    2.0|bought canon vixi...| Negative|2013|   09| 16|   0.0734127|       620|       100|\n",
      "| AEN6KDJ3AJDK6|B000068O4H|    [0, 2]|    2.0|got plug xlr cabl...| Negative|2013|   12| 30|        -0.5|       106|        18|\n",
      "|A3AOB0VF6H0IF4|B000165DSM|    [0, 0]|    1.0|received time sta...| Negative|2013|   01| 27|         0.0|       158|        24|\n",
      "|A27DR1VO079F1V|B000165DSM|    [0, 0]|    1.0|things terrible o...| Negative|2014|   02| 19| -0.40357143|       119|        17|\n",
      "| AAGD3GA9ZVPLQ|B0002CZSJO|    [0, 0]|    2.0|handle spring str...| Negative|2013|   09| 19| 0.036666665|       187|        31|\n",
      "| AH0XON4XDOF2C|B0002CZTIO|    [1, 1]|    2.0|good but think bi...| Negative|2014|   01| 25|         0.1|        73|        13|\n",
      "|A3KBFCPNQ58YK4|B0002CZUUG|    [1, 2]|    2.0|epiphone les paul...| Negative|2014|   02| 24|  0.16435607|       628|       103|\n",
      "|A2B58VXLLOFQKR|B0002CZV82|   [2, 30]|    1.0|cheap piece junk ...| Negative|2009|   11| 13|  0.26666668|       323|        47|\n",
      "|A27L5L6I7OSV5B|B0002CZV82|[142, 150]|    2.0|greg abrams revie...| Negative|2011|   06| 30|  0.23061988|      4676|       713|\n",
      "|A3KBFCPNQ58YK4|B0002CZV82|    [1, 4]|    1.0|heres deal pedal ...| Negative|2014|   02| 21|  0.06845238|       229|        34|\n",
      "|A1M3XD9UV3PD3I|B0002CZVK0|    [0, 2]|    2.0|if want guitar so...| Negative|2014|   01| 28|  0.39166668|       236|        35|\n",
      "|A2X2GEABQXRX7P|B0002CZVXM|    [6, 7]|    1.0|came stock jackso...| Negative|2013|   09| 22|  0.06432292|       525|        82|\n",
      "|A3FW4THIMBIN6V|B0002CZVXM|    [7, 9]|    2.0|disappointed qual...| Negative|2011|   06|  1|-0.021467883|      1604|       228|\n",
      "|A1365RYO0BLEMI|B0002CZVXM|    [0, 2]|    2.0|sorry dont like t...| Negative|2011|   05| 18|  0.18636364|       291|        47|\n",
      "|A18P5I03P4U8AI|B0002CZW0Y|    [0, 0]|    2.0|thing dont seem s...| Negative|2014|   06| 10| -0.15729167|       197|        34|\n",
      "|A10HYGDU2NITYQ|B0002CZW0Y|  [11, 14]|    1.0|purchased reading...| Negative|2011|   04| 19| 0.010416667|       277|        40|\n",
      "+--------------+----------+----------+-------+--------------------+---------+----+-----+---+------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "negative_review.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- asin: string (nullable = true)\n",
      " |-- helpful: string (nullable = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviews: string (nullable = true)\n",
      " |-- Sentiment: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- polarity: float (nullable = true)\n",
      " |-- review_len: integer (nullable = true)\n",
      " |-- word_count: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|Sentiment|\n",
      "+---------+\n",
      "| Positive|\n",
      "| Positive|\n",
      "| Positive|\n",
      "| Positive|\n",
      "| Positive|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Sentiment\").limit(5).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+--------+-------+--------------------+---------+----+-----+---+-----------+----------+----------+\n",
      "|    reviewerID|      asin| helpful|overall|             reviews|Sentiment|year|month|day|   polarity|review_len|word_count|\n",
      "+--------------+----------+--------+-------+--------------------+---------+----+-----+---+-----------+----------+----------+\n",
      "|A2IBPI20UZIR0U|1384719342|  [0, 0]|    5.0|not much write bu...| Positive|2014|   02| 28|       0.25|       162|        25|\n",
      "|A14VAT5EAX3D9S|1384719342|[13, 14]|    5.0|product exactly q...| Positive|2013|   03| 16|0.014285714|       356|        55|\n",
      "|A195EZSQDW3E21|1384719342|  [1, 1]|    5.0|primary job devic...| Positive|2013|   08| 28|     0.1675|       315|        48|\n",
      "|A2C00NNG1ZQQG2|1384719342|  [0, 0]|    5.0|nice windscreen p...| Positive|2014|   02| 14| 0.33333334|       169|        22|\n",
      "| A94QU4C90B1AX|1384719342|  [0, 0]|    5.0|pop filter great ...| Positive|2014|   02| 21|        0.8|       136|        21|\n",
      "|A2A039TZMZHH9Y|B00004Y2UT|  [0, 0]|    5.0|good bought anoth...| Positive|2012|   12| 21|    0.31875|       180|        29|\n",
      "|A1UPZM995ZAH90|B00004Y2UT|  [0, 0]|    5.0|used monster cabl...| Positive|2014|   01| 19|       0.25|       162|        24|\n",
      "| AJNFQI3YR6XJ5|B00004Y2UT|  [0, 0]|    3.0|use cable run out...|  Neutral|2012|   11| 16|0.017651515|       538|        87|\n",
      "|A3M1PLEYNDEYO8|B00004Y2UT|  [0, 0]|    5.0|perfect epiphone ...| Positive|2008|   07|  6|        0.5|       127|        18|\n",
      "| AMNTZU1YQN1TH|B00004Y2UT|  [0, 0]|    5.0|monster makes bes...| Positive|2014|   01|  8|       0.33|       178|        28|\n",
      "|A2NYK9KWFMJV4Y|B00004Y2UT|  [6, 6]|    5.0|monster makes wid...| Positive|2012|   04| 19|  0.0654386|       767|       109|\n",
      "|A35QFQI0M46LWO|B00005ML71|  [0, 0]|    4.0|got if needed fou...| Positive|2014|   04| 22| 0.34166667|       129|        24|\n",
      "|A2NIT6BKW11XJQ|B00005ML71|  [0, 0]|    3.0|if not use using ...|  Neutral|2013|   11| 17|0.013392857|        95|        16|\n",
      "|A1C0O09LOLVI39|B00005ML71|  [0, 0]|    5.0|love used yamaha ...| Positive|2013|   06| 16|       0.65|        58|         8|\n",
      "|A17SLR18TUMULM|B00005ML71|  [0, 0]|    5.0|bought use home s...| Positive|2012|   12| 31|        0.0|        62|        10|\n",
      "|A2PD27UKAD3Q00|B00005ML71|  [0, 0]|    2.0|bought use keyboa...| Negative|2013|   08| 17| 0.26944444|       430|        63|\n",
      "| AKSFZ4G1AXYFC|B000068NSX|  [0, 0]|    4.0|fender cable perf...| Positive|2013|   08| 13| 0.36190477|       261|        39|\n",
      "| A67OJZLHBBUQ9|B000068NSX|  [0, 0]|    5.0|wanted looks alon...| Positive|2013|   07|  9|       0.44|       202|        33|\n",
      "|A2EZWZ8MBEDOLN|B000068NSX|  [3, 3]|    5.0|ive using cables ...| Positive|2013|   03| 18|0.115626104|      1038|       150|\n",
      "|A1CL807EOUPVP1|B000068NSX|  [0, 0]|    5.0|fender cords look...| Positive|2013|   08|  7| 0.56666666|       122|        19|\n",
      "+--------------+----------+--------+-------+--------------------+---------+----+-----+---+-----------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[reviewerID: string, asin: string, helpful: string, overall: double, reviews: string, Sentiment: string, year: string, month: string, day: string, polarity: float, review_len: int, word_count: int]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o204.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 10.0 failed 1 times, most recent failure: Lost task 7.0 in stage 10.0 (TID 24) (DESKTOP-AJHT4E1 executor driver): java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m pdf = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m preprocessing\n\u001b[32m      4\u001b[39m le = preprocessing.LabelEncoder()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus PC\\Desktop\\amazon-realtime-sentiment-analysis\\venv\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202\u001b[39m, in \u001b[36mPandasConversionMixin.toPandas\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    199\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m rows = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) > \u001b[32m0\u001b[39m:\n\u001b[32m    204\u001b[39m     pdf = pd.DataFrame.from_records(\n\u001b[32m    205\u001b[39m         rows, index=\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns=\u001b[38;5;28mself\u001b[39m.columns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    206\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus PC\\Desktop\\amazon-realtime-sentiment-analysis\\venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1263\u001b[39m, in \u001b[36mDataFrame.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1243\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[32m   1244\u001b[39m \n\u001b[32m   1245\u001b[39m \u001b[33;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1260\u001b[39m \u001b[33;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[32m   1261\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m._sc):\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m     sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1264\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus PC\\Desktop\\amazon-realtime-sentiment-analysis\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus PC\\Desktop\\amazon-realtime-sentiment-analysis\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus PC\\Desktop\\amazon-realtime-sentiment-analysis\\venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o204.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 10.0 failed 1 times, most recent failure: Lost task 7.0 in stage 10.0 (TID 24) (DESKTOP-AJHT4E1 executor driver): java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n"
     ]
    }
   ],
   "source": [
    "pdf = df.select(\"*\").toPandas()\n",
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "pdf['Sentiment'] = le.fit_transform(pdf['Sentiment'])\n",
    "df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o901.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 62.0 failed 1 times, most recent failure: Lost task 3.0 in stage 62.0 (TID 164) (DESKTOP-AJHT4E1 executor driver): java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\n\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:286)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\r\n\tat java.io.DataInputStream.readFully(DataInputStream.java:195)\r\n\tat java.io.DataInputStream.readFully(DataInputStream.java:169)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:777)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\n\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:286)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\r\n\tat java.io.DataInputStream.readFully(DataInputStream.java:195)\r\n\tat java.io.DataInputStream.readFully(DataInputStream.java:169)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:777)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[138]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StringIndexer\n\u001b[32m      3\u001b[39m indexer = StringIndexer(inputCol=\u001b[33m\"\u001b[39m\u001b[33mSentiment\u001b[39m\u001b[33m\"\u001b[39m, outputCol=\u001b[33m\"\u001b[39m\u001b[33mSentiment_Indexed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m indexer_model = \u001b[43mindexer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m df = indexer_model.transform(df)\n\u001b[32m      6\u001b[39m df.select(\u001b[33m\"\u001b[39m\u001b[33mSentiment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSentiment_Indexed\u001b[39m\u001b[33m\"\u001b[39m).distinct().show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus PC\\Desktop\\amazon-realtime-sentiment-analysis\\venv\\Lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._fit(dataset)\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    208\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    209\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[32m    210\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus PC\\Desktop\\amazon-realtime-sentiment-analysis\\venv\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:381\u001b[39m, in \u001b[36mJavaEstimator._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) -> JM:\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m     java_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    382\u001b[39m     model = \u001b[38;5;28mself\u001b[39m._create_model(java_model)\n\u001b[32m    383\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._copyValues(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus PC\\Desktop\\amazon-realtime-sentiment-analysis\\venv\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:378\u001b[39m, in \u001b[36mJavaEstimator._fit_java\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_java_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus PC\\Desktop\\amazon-realtime-sentiment-analysis\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus PC\\Desktop\\amazon-realtime-sentiment-analysis\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus PC\\Desktop\\amazon-realtime-sentiment-analysis\\venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o901.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 62.0 failed 1 times, most recent failure: Lost task 3.0 in stage 62.0 (TID 164) (DESKTOP-AJHT4E1 executor driver): java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\n\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:286)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\r\n\tat java.io.DataInputStream.readFully(DataInputStream.java:195)\r\n\tat java.io.DataInputStream.readFully(DataInputStream.java:169)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:777)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\n\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:286)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\r\n\tat java.io.DataInputStream.readFully(DataInputStream.java:195)\r\n\tat java.io.DataInputStream.readFully(DataInputStream.java:169)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:777)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# indexer = StringIndexer(inputCol=\"Sentiment\", outputCol=\"Sentiment_Indexed\")\n",
    "# indexer_model = indexer.fit(df)\n",
    "# df = indexer_model.transform(df)\n",
    "# df.select(\"Sentiment\", \"Sentiment_Indexed\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n",
      "|Sentiment|Sentiment_Indexed|\n",
      "+---------+-----------------+\n",
      "| Positive|              2.0|\n",
      "| Negative|              0.0|\n",
      "|  Neutral|              1.0|\n",
      "+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "data = [(\"Positive\",), (\"Negative\",), (\"Neutral\",)]\n",
    "df2 = spark.createDataFrame(data, [\"Sentiment\"])\n",
    "indexer = StringIndexer(inputCol=\"Sentiment\", outputCol=\"Sentiment_Indexed\")\n",
    "model = indexer.fit(df2)\n",
    "df2 = model.transform(df2)\n",
    "df2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
